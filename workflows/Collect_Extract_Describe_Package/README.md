## Collect_Extract_Describe_Package

This package is designed to process large-scale collections of text and photographic materials, analyze and extract pre-defined information from trained models, and generate summaries and specified metadata from their contents. It uses various controlled and customizable libraries and APIs for text generation, named entity recognition, and project-specific data. 

**Key functionalities:**

* **Scripts:**
    * [**Script #1**](https://github.com/prys0000/congressional-portal-project/blob/d7b4b6fa094744b69b3e2341df11d8d29f1877b5/workflows/Collect_Extract_Describe_Package/allen-free.py), **${\color{green}allen-free}$** contains the imports and libraries for free-open text extraction and .txt file creation.
    * [**Script #1.2**](https://github.com/prys0000/congressional-portal-project/blob/4055ff56fccefa223ea09bf7169726b3bad37a0d/workflows/Collect_Extract_Describe_Package/allen-text.py), **${\color{teal}allen-text}$** contains the imports and libraries for AWS-Txtract paid service to detect text and create .txt.
    * **Script #2**, **${\color{red}gary. py}$** contains the imports and libraries such as spaCy, pandas, and others. It also sets up API keys, directories, and global variables.
    * [**Script #3**](https://github.com/prys0000/congressional-portal-project/blob/4055ff56fccefa223ea09bf7169726b3bad37a0d/workflows/Collect_Extract_Describe_Package/helen.py), **${\color{blue}helen.py}$** contains the imports and libraries preprocesses and transforms data from an Excel file, converting date strings to datetime objects, renaming columns, setting static column values, reordering columns, and saving the updated data to new metadata mapped excel file.
    * [**Script #4](https://github.com/prys0000/congressional-portal-project/blob/4055ff56fccefa223ea09bf7169726b3bad37a0d/workflows/Collect_Extract_Describe_Package/susie.py), **${\color{orange}susie.py}$** checks the validity of links specified in specific columns of an excel file, marks their status (whether they're functioning or not), and saves the updated information into a new Excel file. It also suppresses the InsecureRequestWarning generated by the requests module when dealing with SSL verification.

* **Text Processing Functions:**  There are various functions that process and extract information from text files. These functions handle tasks like extracting dates, naming entities, tribes, and states, generating titles and summaries, and identifying the nature of the documents or photographs (constituent correspondence, form, memo, telegram, published work, and tone).

* **DataFrame Creation:**  The collected data is organized into a pandas DataFrame for easy manipulation and analysis. Additional information, such as Congress based on dates, is also included.

* **Quality Control and Error Detection:** After the DataFrame is created, each component is validated for the accuracy or potential errors with spelling, word organization, dates and years, and related legislation and congressional date ranges, and validates tribal affiliation accuracy (project-specific metric).

* **Export to Excel:**  The data frames are saved as an Excel file for further analysis or reporting. (*note: if the conversion of file types is specified, those items are also printed to the directory*)

* **Transform data for ArchivesSpace, Preservica, and NEH Congressional Portal:** There are several multi-step process that involves reading data from an Excel sheet, performing various checks and validations, highlighting potential errors, and ultimately transforming the data into platform-specific bulk uploads.

* **Data Validation and Review:**  After retrieving the data, validation checks are performed to ensure data accuracy and consistency. For example, they check for missing or incorrect entries, missing metadata, or inconsistent date formats.

* **Spell Check:**  Spell checks are employed to identify and highlight potential spelling errors in the data. This ensures that the textual content is free from typos or misspelled words.

* **Error Identification:** Errors are identified, highlighting other potential errors or inconsistencies within the data. These errors could be related to metadata, formatting, or other relevant criteria.

* **Data Transformation:** Once the data has been validated and potential errors are addressed, it is transformed. Transformation involves converting the data into a format compatible with various archival platforms. This may include structuring the data hierarchically, generating XML files, and preparing files for upload.

* **Platform-Specific Bulk Uploads:**  The transformed data is tailored to meet the requirements of specific archival platforms. Each platform may have its format and structure for data uploads, and the script ensures compliance.

* **XML Creation:**  For each folder or item, generate XML files that contain metadata and other necessary information for archival purposes. These XML files serve as structured representations of the resources.

* **Hierarchy Maintenance:** Hierarchical structure is maintained to preserve the data and the relationships between items, folders, and collections.

* **Preservation and Compression:**  Preservation folders store archival resources in their original state, along with relevant metadata. These preservation folders are typically "bagged" according to archival standards to ensure long-term data preservation.

* **Queuing for Long-Term Storage:**  Finally, the preservation folders for long-term storage ensure that the archival resources are securely stored and accessible for the future.

* **Customizability:**  One of the key features of the script is its ***customizability***.
    * Users can configure various parameters and settings to adapt the data transformation and upload process to their specific needs and the requirements of different archival platforms.

